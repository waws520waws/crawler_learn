【https://baijiahao.baidu.com/s?id=1660689423524423511&wfr=spider&for=pc】
## 1、动态数据(Ajax、js动态渲染)

## 2、反爬
比方返回假数据、返回图片化数据、返回乱序数据、返回骂人的数据、返回求饶的数据；
反爬也得当心点，之前见过一个反爬直接返回 命令行 rm -rf 的也不是没有，你要是正好有个脚本模仿执行返回结果，结果就可以跑路了

## 3、浏览器模拟工具（自动化）
Puppeteer（js版）、Pyppeteer（py版的Puppeteer）、Selenium、Splash

## 爬虫代码 或者 框架（如scrapy、pyspider）怎样部署

## 抓包工具
chales、fiddler、mitmproxy

## 4、并发
多进程、多线程、协程
异步协程，结合使用 aiohttp、gevent、tornado 等插件

## 5、避免网站封你 IP、封你账号、弹验证码、返回假数据

## 6、分布式爬虫
如今主流的 Python 散布式爬虫还是基于 Scrapy 的，对接 Scrapy-Redis、Scrapy-Redis-BloomFilter 或者用 Scrapy-Cluster 等等，
他们都是基于 Redis 来共享爬取队列的，总会多几少遇到一些内存的问题。所以一些人也思索对接到了其他的消息队列上，比方 RabbitMQ、Kafka 等等，其实效果也差不多少。
总之，想大规模、批量、高效的采集数据，分布式是必不可少的。

评估多少台机器合适


## 8、验证码识别
PIL库、OpenCV

## 9、封 IP
云主机作为代理服务器(adsl拨号、vps)；代理池

## 10、封账号
1、看看别的终端，比方手机页、App 页、wap 页，看看有没有能绕过登录的法子；
2、比较好的办法，那就是分流。假如你号足够多，建一个池子，比方 Cookies 池、Token 池、Sign 池反正不论什么池吧，
多个账号跑出来的 Cookies、Token 都放到这个池子里面，用的时分随机从里面拿一个。
假如你想保证爬取效率不变，那么 100 个账号相比 20 个账号，关于每个账号对应的 Cookies、Token 的取用频率就变成原来的了 1/5，那么被封的概率也就随之降低了。

## 11、JavaScript 逆向
用于解决加密，这一步是比较有难度的


## 12、App客户端
Appium、抓包工具（mitmproxy、Fiddler、Charles）
逆向、脱壳技术、反编译、混淆

## 13、数据库
msyql的增删改查、mongodb、redis、elasticsearch。  
数据库调优、海量数据存储。  
集群，分布式存储；  
数据保存肯定要会数据库的。不过有时候一些小数据也可以保存成json或者csv等。我有时想抓一些图片就直接按照文件夹保存文件。
推荐使用NoSQL的数据库，比如mongodb，因为爬虫抓到的数据一般是都字段-值得对应，有些字段有的网站有有的网站没有，mongo在这方面比较灵活，况且爬虫爬到的数据关系非常非常弱，很少会用到表与表的关系

## 14、HTTP知识/网络编程
- 【参考】：https://www.bilibili.com/video/BV1434y1U7B9?p=43&spm_id_from=pageDriver  
（url请求到返回的整个过程）
HTTP协议要理解。HTTP协议本身是无状态的，那么“登录”是怎么实现的？这就要求去了解一下session和cookies了。
GET方法和POST方法的区别（事实上除了字面意思不一样没有任何区别）。
浏览器要熟练。爬虫的过程其实是模拟人类去浏览器数据的过程。所以浏览器是怎么访问一个网站的，你要学会去观察，怎么观察呢？
Developer Tools！Chrome的Developer Tools提供了访问网站的一切信息。从traffic可以看到所有发出去的请求。
copy as curl功能可以给你生成和浏览器请求完全一致的curl请求！
我写一个爬虫的一般流程是这样的，先用浏览器访问，然后copy as curl看看有哪些header，cookies，然后用代码模拟出来这个请求，最后处理请求的结果保存下来
  

## 15、运维
爬虫的采集和更新策略
提升爬虫的性能（效率、质量），占用资源情况
linux的调优监控

## 16、数据的去重
url去重、内容去重（增量式爬虫）

## 17、爬虫框架
scrapy、pyspider、scrapyd等  
分布式爬虫框架：scrapy-redis、celery

## 18、爬虫的监控和部署

## 19、提高爬虫效率
- 多进程、多线程、异步协程、分布式、scrapy提高爬取速度
- Python建立数据库连接池提高效率

## 20、浏览器插件的使用
- chrome插件 Web Scraper 抓取数据

## 21、验证码提供平台
极验、网易易盾。  
网上应该有教程破解这些平台提供的验证码

## 22、云打码平台
超级鹰、百度ai

## 23、docker、部署容器化爬虫
简单地说，就是将你的项目和依赖包(基础镜像)打成一个带有启动指令的项目镜像，然后在服务器创建一个容器，让镜像在容器内运行，从而实现项目的部署。  
【b站视频】https://www.bilibili.com/video/BV1r541137ir?p=56

## 24、搜索引擎
通用爬虫：url就要涉及到所有网页，这个‘所有’是如何做到的：  
    新网站向搜索引擎主动提交网址；  
    在其他网站上设置新网站外链；  
    搜索引擎和DNS解析服务商(如DNSPod等)合作，新网站域名将被迅速抓取。

## 25、瑞数加密
- 是对cookie的参数进行加密（动态的）？

## 其他：
- 1、要抓十几个相似度很高但是html结构不太一样的网站，我就写了一个简单的代码生成器，从爬虫代码到单元测试代码都可以自动生成，只要对应html结构稍微修改一下就行了。
- 2、我们需要一种任务队列，它的作用是：讲计划抓取的网页都放到任务队列里面去。然后worker从队列中拿出来一个一个执行，如果一个失败，记录一下，然后执行下一个。
  这样，worker就可以一个接一个地执行下去。也增加了扩展性，几亿个任务放在队列里也没问题，有需要可以增加worker，就像多一双亏筷子吃饭一样。
- 常用的任务队列有kafka，beanstalkd，celery等
- 多上github
- 网上觉得麻烦的，都可以考虑用爬虫解决（爬虫思维）
- 可以学习web知识。可视化用的是Django框架，表格展示数据用的是前端Datatable框架，各种图表展示用的是前端的echarts框架。数据分析方面，会Pandas库。
- 如何提升爬虫的性能（效率、质量）
- 抓取主流平台
- 抓取App很重要
- 爬虫应用（哪些地方可以用爬虫）
    - https://mp.weixin.qq.com/s?__biz=MzA5MjA3MjMxNw==&mid=2247483704&idx=1&sn=4b079252b6e5a520993ac1a4762be3be&chksm=9073f12aa704783c66e3a4bca5b3de12f620a80e35589e3667831df62c1138b3c25f38908f85&scene=21#wechat_redirect
- 关于逆向的文章：去看雪论坛或者吾爱破解论坛吧，哪里有太多的文章
- 掘金网、github找技术
- 分布式搜索引擎「Elasticsearch」、分布式文件系统「HDFS」、分布式消息队列「Kafka」、HBase

## 如何成为高级爬虫工程师
- 【参考】：https://www.bilibili.com/video/BV1434y1U7B9?p=131
- 自动化部署和监控
- 应用相关
- 分布式存储
- 进一步解决反爬
- 进一步的解析方案
- 自己设计架构
- 多多的关注一些前端的开发技术，这样可以更好的了解到网站的基本结构，基本开发技巧，实现更优的爬虫手段

# 面试
- 爬过哪些网站
- 技术方面：
    - 1、反爬、代理、去重、增量、部署
        - 1）反反爬
            - 设置延时，合理控制速度
            - 伪装成浏览器，设置请求头
            - 设置cookie，登录状态
            - ip代理
                - 高质量的ip贵
                - 隧道动态代理ip
            - ADSL拨号
                - 需要租一台拨号服务器（vps），相对于用高质量的ip要便宜些
                - https://zhuanlan.zhihu.com/p/25286144
            - 云打码平台处理验证码（）
            - js逆向解决加密、js反混淆
            - 封账号
            - 页面跳转
        - 2）代理
            - 构建ip代理池
        - 3）app逆向
            - 绕过证书验证类
            - 反编译
            - 反混淆
            - 脱壳
    - 2、多线程、多进程、异步协程；线程池、进程池
        - 线程数，时间间隔
    - 3、HTML、CSS、JS、DOM、tcp/ip、ajax工作原理
        - DOM
            - DOM (Document Object Model) 译为文档对象模型，是 HTML 和 XML 文档的编程接口；
            - DOM 以树结构表达 HTML 文档，可以方便地访问、修改、添加和删除DOM树的结点和内容
        - tcp/ip协议（传输控制协议/网际协议）
            - 主要经历以下4个步骤：应用层，传输层，Internet层，物理层。
            - 假如你给你的基友发一个消息，数据开始传输，这时数据就要遵循TCP/IP协议啦，你的电脑会做出以下动作，这些动作你是看不到的。
            - 1）应用层先把你的消息进行格式转换,你的消息是文字还是图片，还是成人视频并进行加密等操作交给传输层。（这时的数据单元（单位）是信息）
            - 2）传输层将数据切割成一段一段的，便于传输并往里加上一些标记，比如当前应用的端口号等，交给Internet。（这时的数据单元（单位）是数据流）
            - 3）Internet层开始在将数据进行分组，分组头部包含目标地址的IP及一些相关信息交给物理层。（这时的数据单元（单位）是分组）
            - 4）物理层将数据转换为比特流开始查找主机真实物理地址进行校验等操作，校验通过，开始嗖~嗖~嗖~的住目的地跑。（这时的数据单元（单位）是比特）
            - 到达目的地后，对方设备会将上面的顺序反向的操作一遍，最后呈现出来。
        - ajax工作原理
            - Ajax（Asyncchronous JavaScript and Xml），翻译过来就是说：异步的javaScript和xml
            - 在不需要重新刷整个页面的情况下，Ajax 通过异步请求加载后台数据，并在网页上产生局部刷新的效果
              
            - Ajax的工作原理相当于在用户和服务器之间加了一个中间层(Ajax引擎)，使用户操作与服务器响应异步化。并不是所有用户请求都提交服务器。
              像一些数据验证（表单验证是否登入成功）和数据处理等都交给Ajax引擎自己来做，只有确定需要从服务器读取新数据时再由Ajax引擎代为向服务器提交请求
              
            - Ajax与服务器通信有一个核心的对象 XMLHttpRequest
            - 例如视频播放网站，首次播放时（静态内容与动态内容分别请求加载）会加载整个页面，很多信息，很多请求（html文档、图片、js脚本、音乐、广告），
              再点击下一集时只有视频播放区域局部刷新，就不需要再加载这些静态内容了
    - 4、爬虫库
        - requests
    - 5、数据提取
        - 正则表达式、xpath提取页面元素（或者bs4、pyquery）
    - 6、框架
        - scrapy
    - 7、数据库
        - [mysql的增删改查, 特别是查]、mongodb、redis
        - 数据备份（全量备份与增量备份）
        - 选用mongodb的原因：
            - 适合大规模数据（支持分片）
            - 爬取的内容有图文，mongodb使用键值对，更合适
            - 关系型数据库要确定字段的类型，varchar 类型数据还要定义长度，你定义的小了，数据太长就会截断（又因为新闻正文内容长度不确定）
            - 刚刚抓下来的数据，通常需要二次清洗才能使用，如果你用关系型数据库存储数据，第一次就需要定义好表结构，清洗以后，恐怕还需要定义个表结构，
              将清洗后的数据重新存储，这样过于繁琐，使用mongodb，免去了反复定义表结构的过程。
        - 在实际的生产环境中，MongoDB 基本是以集群的方式工作的。有三种集群部署模式，分别为主从复制（Master-Slaver）、副本集（Replica Set）和分片（Sharding）模式。
        - redis如何实现持久化
            - https://www.cnblogs.com/chenliangcl/p/7240350.html
    - 8、linux
        - 熟悉Linux日常工作环境，熟练掌握常用命令和调优监控手段
            - 在系统层面能够影响应用性能的一般包括三个因素：CPU、内存和IO
            - 查看CPU使用情况：top、vmstat、ps等命令
    - 9、git
        - 团队协作开发工具git的熟练使用（版本控制）
    - 10、数据清洗、数据分析
        - python科学计算库numpy、scipy和数据分析库pandas的熟练使用
            - pandas
                - dropna()
                - fillna()
                - mean()均值、median()中位数、mode()众数
                - duplicated()
                - groupby()
    - 11、熟悉http、https原理，http状态码，理解cookie机制
    - 12、文本、图片、视频
    - 13、抓包工具
    - 14、APP相关
        - 逆向
        - appium自动化工具（类似于selenium）
    - 15、之前学过的，挨着查看笔记
    
- 上个公司
    - 0）在上个公司中，项目遇到最多的反爬就是 Cloudflare反爬(5秒盾)
        - 解决办法
            - 1）之前使用的是 cloudscraper库（or cfscrape库？）
                - 【参考】https://blog.csdn.net/qq_40244755/article/details/115914539
                - 这个网站有设置Cloudflare反爬：https://www.today.ng/sport/basketball
                - 存在的问题：cloudscraper库要付费，这种方法行不通
            - 2）现在：pyppeteer（py版的puppeteer）
            - 3）selenium、抓包分析逆向
    - 1）新闻网页是怎么存的，杂质（换行标签，read more）怎么处理（python怎么实现）
        - 要存的东西
            - 域名，详情页url，url正则orXpath，title_Xpath，时间xpath，内容xpath，DustXpath，replaceTag，replaceAttr，保存标签以及标签的有用的属性或文本，
                所属新闻类别（娱乐、军事等）（url中一般有类别关键字，是根据这个来分的？）
        - 杂质处理
            - 与用js写的思路相同【配置dustXpath，然后过滤这些节点；也可以使用pyquery删除这些节点】
        - 特殊杂质处理
            - 有这些杂质的网页单独处理
            - 清理连续的换行，清理与换行相连的空段落，处理非连续的两个相同Ins控件，处理svg、gif等图片，处理被抓为文字的代码，处理相对路径
    - 2）公司是抓取新闻网页，那么异步的数据（比如插件）怎么办呢，而且要保持原文的格式；
        - 查看了好多网页，源码都有插件（难道不是异步的，但是打开网页很明显插件是后加载出来的）
    - 3）用python写怎么保持原新闻网页的格式段落
        - 公司是用js写的，python也可以实现（我试过），与用js写的思路相同
    - 4）如何处理页面跳转
- 项目经历
    - 超时时间、异常处理、并发量、爬虫时间间隔
    
    - 爬过的网站
        - 虎嗅网：100例例57
            - 滑动验证码，selenium，mitmproxy
        - 抖音app
                                    - 出处：https://www.bilibili.com/video/BV1r541137ir?p=47&spm_id_from=pageDriver
            - 需求：拿到用户的抖音id、分享id、用户名、粉丝数、获赞数；并继续遍历用户的关注列表里的用户
            - 技术：Xposed框架与JustTrustme组件绕过SSL证书验证；使用mitmproxy获取响应数据（此方法可以不用破解参数加密）；
                   appium模拟滑动操作；多设备并发抓取抖音粉丝数据
        - 某APP
            - 教程时间：2021-5-11
            - 出处：https://zhuanlan.zhihu.com/p/64901503
            - 前段时间做爬虫遇到一个app，里面的数据需要登录之后才能拿到，而且登录不能用密码，只能通过验证码登录。
            - 破解 token 的参数，APP逆向之反编译、DES加密
        - facebook视频
    - 爬虫监控（状态、爬取数量、资源占用情况等）、爬虫日志
        - 使用logging模块编写监控程序进行爬虫监控，并根据日期定向输出日志到log文件
        - 邮件监控smtp（scrapydweb自带邮件以及日志可视化监控）
    
- 离职理由
- 背调
    - 于洲 18780062818
    - 曾叙平 15082386093
    - 汤浩 18615789588
    - 蔡英珏 13438281560
    - 陈凤凯 15228274892
# 职场
- 多问多交流，主动找事做
- 不只要埋头苦干，也要让领导看到你的发光点,你的努力
- 多做笔记
- 明白需求 -》可行性分析 -》理性思路 -》干
- 考虑公司的利益、费用（爬虫支出）