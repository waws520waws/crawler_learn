### 一、一些爬虫教程

- 猿人学：https://www.yuanrenxue.com/crawler/high-performance-crawler.html
- B站
- CSDN 爬虫100例

### 二、爬虫技巧

#### 1、Web爬取不了就爬取移动端网页

#### 2、破解一家产品，成本从低到高思路如下：

  - 先看是否提供API
  - web端 < 公众号 < 小程序 < app(安卓) < app(IOS)
  - 在抓数据之前，使用和熟悉被抓对象的产品形态（PC，H5，APP）和功能，首先看是否有web页面【比如访问链接（如抖音app有分享链接，可直接在web上访问）】
  - 在爬任何数据的时候，一定不要先考虑用Appium（效率低）

#### 3、开始爬虫前正确的做法应该是：

  - 1.去百度和谷歌搜下这个网站有没有人分享出你要爬数据的API
  - 2.看看电脑网页有没有你要的数据，调查下好不好拿，不管好不好拿，也不要急着就开爬
  - 3.看看有没有电脑能打开的手机网站，一般格式为http://m.xxx.com或http://mobile.xxxx.com，有的话可以用F12检查抓下包，看下抓取难易程度
  - 4.看看有没有手机App，抓下App的包，看能不能抓到接口
  - 5.抓下公众号和小程序的包，看能不能抓到接口

#### 4、抓取手机app时，参数是加密的，怎么办？

- 方法1：app逆向，找源码，找加密规则代码
- 方法2：利用mitmproxy，携带python脚本（编写代码：将返回的数据解析并写入本地文件）
  - 【参考】https://www.bilibili.com/video/BV1r541137ir?p=47&spm_id_from=pageDriver

#### 5、绕开登录和访问频率

- 1.设置请求头referer。referer是告诉目标服务器(访问的网站)，你是从哪儿点击进入当前页面的
- 2.看下robots.txt文件。
  - 例子1：老板给你布置一个任务，把豆瓣每天新产生的影评抓取下来。初想一下，这任务得有多大，豆瓣有1.6亿注册用户，每个人的主页你至少每天要访问一次。
    - 解决方法：看下豆瓣的robots.txt，可以看到有个 sitemap_updated_index.xml 文件，里面是一个个压缩文件，文件里面是豆瓣头一天新产生的影评，书评，帖子等等
  - 例子2：发现大量URL例子，把天眼查上的几千万家企业工商信息抓取下来。但是分析网站后发现这类网站的抓取入口很少(抓取入口是指频道页，聚合了很多链接的那种页面)
    - 解决方法：看下天眼查的robots.txt，发现有个 everr_www.xml 的文件，打开，里面有大量公司的URL，且URL是根据年月日生成的

#### 6、大规模抓取

##### 1）需要考虑的问题

​	【参考】https://www.yuanrenxue.com/crawler/high-performance-crawler.html

​	IP代理池、adsl拨号（vps）、网络性能、抓取技术细节调优
​	管理DNS缓存 https://www.yuanrenxue.com/crawler/crawler-dns.html

##### 2）如何抽取上千家新闻网站正文

​	【参考】https://www.yuanrenxue.com/crawler/extract-page.html

​	法1：上千个网址，可想而知，如果单纯的使用 xpath 进行解析，那就只能是望洋兴叹。经过一段时间的研究，在论文海中排查出了今天的主角––基于文本及符号密度的网页正文提取方法，经过测试，准确率符合项目的实际开发需求。

​	法2：Readability，网页提取中,应用最广泛的就是Readability，该算法需要解析DOM树,因此时间复杂度和空间复杂度较高，提取网页正文的时间比较长。

### 三、写爬虫需要考虑的东西
- 1、去重
    - url去重、内容去重
- 2、效率、占用资源、费用
- 3、数据备份
    - 例如：每天做一次增量备份，每周做一次全量备份
- 4、爬虫监控
- 5、对于批量url，请求失败的url如何处理、记录
- 6、程序中途出错，如何保证每次启动都是爬剩下的而不是从头开始重复爬
- 7、请求失败得重试（可以是在 10 秒后重试，然后在 20 秒后重试，然后一分钟等等）
- 效率
- 并发量
- 超时时间
- 爬虫时间间隔
- 进入子程序的判断语句（如：if response.status == 200）
- **异常捕获**
- 面向对象
- 数据清洗
- 其他注意的地方
  - https://www.cnblogs.com/stlong/p/11223551.html
  - 每次启动都是爬剩下的而不是从头开始重复爬
- 爬虫程序所依赖环境可以写在一个 txt 文件中，以及一个 README.md 文件，可参考 github 上的

### 四、踩过的坑
- 多线程
  - 使用多线程未处理异常，导致存活的线程越来越少，爬取的越来越慢
  - 多次保存数据到文件，导致速率慢
- 使用requests请求（或者其他模块请求），要加超时时间，否则会卡住

### 五、数据库
- 加入日期时间字段

### 六、优化

#### 1、硬盘存储

- 对于存储需求大的，可以掐头去尾（如html只保存body里的内容），在保存

#### 2、内存

- 做爬虫程序为了防止重复抓取URL，一般要把URL都加载进内存里
  - 可以不保存完整地址
  - 可以**使用BloomFilter算法**，原理是把一个字符串映射到一个bit上，一个url只占用1个bit（1字节=8bit）
