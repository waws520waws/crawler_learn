### 【问题1】若请求头中有 `Connection: keep-alive` 参数
- 此参数的作用：
    - 持久连接
    - 参考：https://segmentfault.com/q/1010000019503002
- 【解决方法】要通过 `requests.session()` 去发起请求
  - 使用
      ```python
        import requests 
        #实例化session
        session = requests.session()
        # 使用session发起请求来获取登录后的cookie,cookie已经存在session中
        response = session.get(url, headers=req_header)
      ```
  - session 会话的作用：
    - 在一定时间内多次请求同一个网站，不用每次都登陆（会默认使用该session之前使用的cookie等参数）

### 一、cookie
#### 1、怎么自动获取 cookie
- 【解决方法1】requests 中的 session()，如上
- 【解决方式2】通过 selenium 获取
- 【解决方式3】通过js逆向找到加密参数，构造出cookie
- 【解决方式4】利用fiddler hook住cookie中的变量，参考 `JS逆向/hook.md`

#### 2、辨别 cookie 来源
可以看浏览器控制台cookie中的 httpOnly 这一栏，如果有 √ 的是来自于服务端，如果没有 √ 的话是本地生成的

#### 2、判断 cookie 中哪些参数是加密的
- 从服务器返回的参数会出现在 response headers 中的 `set-cookie` 参数中
- 没有在 `set-cookie` 的参数中，则是加密的

#### 3、cookie字符串转字典
```python
import requests
cookie_str = ""
li = cookie_str.split('; ')
cookie_dic = {}
for i in li:
    k, v = i.split('=', 1)
    cookie_dic[k] = v
```

### 4、scrapy中如何设置 cookie
- 【注意】scrapy中设置cookie时必须是 字典格式
- 方法1：
    - middlewares中设置cookie
        - 在middlewares中的 downloadermiddleware 中的 process_request 中配置cookie，配置如下：
            request.cookies={
              '':'',
              '':'',
            }
    
- 方法2：
    - 在spider爬虫主文件中，重写start_request方法，在scrapy的Request函数的参数中传递cookies
```python
import scrapy
# 重载start_requests方法
def start_requests(self):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:59.0) Gecko/20100101 Firefox/59.0"
    }
    # 指定cookies
    cookies = {
        'uuid': '66a0f5e7546b4e068497.1542881406.1.0.0',
        '_lxsdk_cuid': '1673ae5bfd3c8-0ab24c91d32ccc8-143d7240-144000-1673ae5bfd4c8',
        '__mta': '222746148.1542881402495.1542881402495.1542881402495.1',
        'ci': '20',
        'rvct': '20%2C92%2C282%2C281%2C1',
        '_lx_utm': 'utm_source%3DBaidu%26utm_medium%3Dorganic',
        '_lxsdk_s': '1674f401e2a-d02-c7d-438%7C%7C35'
    }

            # 再次请求到详情页，并且声明回调函数callback，dont_filter=True 不进行域名过滤，meta给回调函数传递数据
    yield scrapy.Request(detailUrl, headers=headers, cookies=cookies, callback=self.parse, meta={'myItem': item},  dont_filter=True)
```

### 二、请求头中有 _token 参数
- `_token` 参数一般出现在 PHP 框架 laravel 框架中，出现它表示你需要获取到一个 cookie 中的参数才可以继续爬取，参数名为 laravel_session
- Token是服务端生成的一串字符串，以作客户端进行请求的一个令牌，当第一次登录后，服务器生成一个Token便将此Token返回给客户端，
  以后客户端只需带上这个Token前来请求数据即可，无需再次带上用户名和密码。

### 三、IP代理池
- 概念：将网上的ip保存到本地文件或者数据库中，需要用的时候从中取出
- 访问免费代理的网站 —> 正则/xpath提取 ip和端口 —> 测试ip是否可用 —> 可用则保存 —> 使用ip爬虫 —> 检查过期，抛弃ip
- 定期的去检测这些 ip 可不可以用，那么下次你要使用代理 ip 的时候，你只需要去自己的 ip 代理池里面拿就行了
- 开源 ip代理池 `ProxyPool`
    - 【参考】https://blog.csdn.net/weixin_44517301/article/details/103393145
    - ProxyPool下载地址：https://github.com/Python3WebSpider/ProxyPool.git
    - 会抓取某页面上的有效的ip
- 测试代理是否成功可以访问这个网站：http://icanhazip.com，这个网站会返回当前请求的ip地址，来测试代理ip是否配置成功
```python
import requests
url = 'http://icanhazip.com'
proxy = {
    # "待请求网站的协议类型" : "[https://]ip:port"
    "http":"http://35.224.248.29:3128",
    'https': 'http://STHB9U24:62E81DC42692@36.6.140.218:18220'
}
response = requests.get(url,proxies=proxy)
print(response.text)
```
- 构建ip代理池的方法：https://steven-cloud.blog.csdn.net/article/details/102538757
  - 构建代理池接口的作用：运行flask后，我们每次需要代理IP时，只需要请求flask中设置的路径，就可得到一个IP（而️不需求每次都连接数据库，再取出）

### 四、header格式化
- 使用 Notepad++ 工具的替换功能，查找 `(.*?): (.*)`, 正则替换为：`('$1'): ('$2'),`